{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ZeroPadding2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TerminateOnNaN, TensorBoard\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV data\n",
    "train_data = pd.read_csv(r\"D:\\Projects\\Digit-Recognizer-ML_Flask\\train.csv\")\n",
    "\n",
    "# Prepare CSV dataset\n",
    "x_csv = train_data.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_csv = to_categorical(train_data['label'].values)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_mnist_train, y_mnist_train), (x_mnist_test, y_mnist_test) = mnist.load_data()\n",
    "x_mnist_train = x_mnist_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_train = to_categorical(y_mnist_train, 10)\n",
    "\n",
    "x_mnist_test = x_mnist_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_test = to_categorical(y_mnist_test, 10)\n",
    "\n",
    "# Concatenate CSV and MNIST datasets\n",
    "x_combined = np.concatenate([x_csv, x_mnist_train, x_mnist_test], axis=0)\n",
    "y_combined = np.concatenate([y_csv, y_mnist_train, y_mnist_test], axis=0)\n",
    "\n",
    "# Train-validation split for combined dataset\n",
    "random_seed = 2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_combined, y_combined, test_size=0.1, random_state=random_seed)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    ZeroPadding2D(padding=(1, 1), input_shape=(28, 28, 1)),  # Input layer, padding added\n",
    "    Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Activation(activations.relu),  # Activation layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),  # Max pooling layer\n",
    "    ZeroPadding2D(padding=(1, 1)),  # Zero padding layer\n",
    "    Dropout(0.2),  # Dropout layer\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Activation(activations.relu),  # Activation layer\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Max pooling layer\n",
    "    Dropout(0.2),  # Dropout layer\n",
    "    \n",
    "    Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Activation(activations.relu),  # Activation layer\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Max pooling layer\n",
    "    Dropout(0.3),  # Dropout layer\n",
    "    \n",
    "    Flatten(),  # Flatten the 3D outputs to 1D\n",
    "    Dense(128, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.4),  # Dropout layer\n",
    "    Dense(10, activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Callbacks for saving the best model and learning rate reduction\n",
    "checkpoint = ModelCheckpoint(\n",
    "    r'best_model.keras', \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True, \n",
    "    verbose=1,\n",
    "    mode='auto'\n",
    ")\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', \n",
    "    patience=3, \n",
    "    verbose=1, \n",
    "    factor=0.5, \n",
    "    min_lr=0.00001\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Моніторинг валідаційних втрат\n",
    "    patience=5,                # Чекати 5 епох без покращення перед зупинкою\n",
    "    restore_best_weights=True,  # Відновити ваги найкращої моделі\n",
    "    mode='auto'\n",
    ")\n",
    "terminate_nan = TerminateOnNaN()\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./public',         # Директорія для збереження логів\n",
    "    histogram_freq=1,        # Записувати гістограми змінних після кожної епохи\n",
    "    write_graph=True,        # Записувати граф моделі\n",
    "    write_images=True,      # Записувати зображення ваг\n",
    "    update_freq='epoch'     # Оновлення на основі кожної епохи\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Image Data Generator for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "   rotation_range=10,          # Randomly rotate images by up to 10 degrees\n",
    "   zoom_range=0.1,             # Randomly zoom images by up to 10%\n",
    "   width_shift_range=0.1,      # Randomly shift images horizontally by up to 10%\n",
    "   height_shift_range=0.1,     # Randomly shift images vertically by up to 10%\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 100\n",
    "history = model.fit(datagen.flow(x_combined, y_combined, batch_size=batch_size),\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    steps_per_epoch=x_combined.shape[0] // batch_size,\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint, learning_rate_reduction, early_stopping, terminate_nan, tensorboard],\n",
    "                    shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_val, Y_val)\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\my-course\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7333 - loss: 0.8057\n",
      "Epoch 1: val_accuracy improved from -inf to 0.98491, saving model to 111.keras\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 98ms/step - accuracy: 0.7334 - loss: 0.8053 - val_accuracy: 0.9849 - val_loss: 0.0487 - learning_rate: 0.0010\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\my-course\\lib\\contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.98491\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9849 - val_loss: 0.0487 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9625 - loss: 0.1359\n",
      "Epoch 3: val_accuracy improved from 0.98491 to 0.98679, saving model to 111.keras\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 95ms/step - accuracy: 0.9625 - loss: 0.1359 - val_accuracy: 0.9868 - val_loss: 0.0436 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.98679\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9868 - val_loss: 0.0436 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9731 - loss: 0.1000\n",
      "Epoch 5: val_accuracy improved from 0.98679 to 0.99116, saving model to 111.keras\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 95ms/step - accuracy: 0.9731 - loss: 0.1000 - val_accuracy: 0.9912 - val_loss: 0.0332 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.99116\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9912 - val_loss: 0.0332 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9766 - loss: 0.0865\n",
      "Epoch 7: val_accuracy did not improve from 0.99116\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 102ms/step - accuracy: 0.9766 - loss: 0.0865 - val_accuracy: 0.9887 - val_loss: 0.0408 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.99116\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9887 - val_loss: 0.0408 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9838 - loss: 0.0640\n",
      "Epoch 9: val_accuracy improved from 0.99116 to 0.99277, saving model to 111.keras\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 97ms/step - accuracy: 0.9838 - loss: 0.0640 - val_accuracy: 0.9928 - val_loss: 0.0272 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.99277\n",
      "\u001b[1m1120/1120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9928 - val_loss: 0.0272 - learning_rate: 5.0000e-04\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9921 - loss: 0.0324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02721174992620945, 0.992767870426178]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TerminateOnNaN, TensorBoard\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load CSV data\n",
    "train_data = pd.read_csv(r\"D:\\Projects\\Digit-Recognizer-ML_Flask\\train.csv\")\n",
    "\n",
    "train_data = train_data.astype('float32')\n",
    "\n",
    "# Prepare CSV dataset\n",
    "x_csv = train_data.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_csv = to_categorical(train_data['label'].values)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_mnist_train, y_mnist_train), (x_mnist_test, y_mnist_test) = mnist.load_data()\n",
    "x_mnist_train = x_mnist_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_train = to_categorical(y_mnist_train, 10)\n",
    "\n",
    "x_mnist_test = x_mnist_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_test = to_categorical(y_mnist_test, 10)\n",
    "\n",
    "# Concatenate CSV and MNIST datasets\n",
    "x_combined = np.concatenate([x_csv, x_mnist_train, x_mnist_test], axis=0)\n",
    "y_combined = np.concatenate([y_csv, y_mnist_train, y_mnist_test], axis=0)\n",
    "\n",
    "# Train-validation split for combined dataset\n",
    "random_seed = 42\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_combined, y_combined, test_size=0.1, random_state=random_seed)\n",
    "\n",
    "# Building a CNN model\n",
    "input_shape = (28,28,1)\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "# Image Data Generator for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "   rotation_range=10,          # Randomly rotate images by up to 10 degrees\n",
    "   zoom_range=0.1,             # Randomly zoom images by up to 10%\n",
    "   width_shift_range=0.1,      # Randomly shift images horizontally by up to 10%\n",
    "   height_shift_range=0.1,     # Randomly shift images vertically by up to 10%\n",
    ")\n",
    "\n",
    "# layer 1\n",
    "x = Conv2D(64,(3,3),strides=(1,1),name='layer_conv1',padding='same')(X_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2,2),name='maxPool1')(x)\n",
    "# layer 2\n",
    "x = Conv2D(32,(3,3),strides=(1,1),name='layer_conv2',padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2,2),name='maxPool2')(x)\n",
    "# layer 3\n",
    "x = Conv2D(32,(3,3),strides=(1,1),name='conv3',padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2,2), name='maxPool3')(x)\n",
    "# fc\n",
    "x = Flatten()(x)\n",
    "x = Dense(64,activation ='relu',name='fc0')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(32,activation ='relu',name='fc1')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(10,activation ='softmax',name='fc2')(x)\n",
    "\n",
    "conv_model = Model(inputs=X_input, outputs=x, name='Predict')\n",
    "\n",
    "# Callbacks for saving the best model and learning rate reduction\n",
    "checkpoint = ModelCheckpoint(\n",
    "    r'111.keras', \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True, \n",
    "    verbose=1,\n",
    "    mode='auto'\n",
    ")\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', \n",
    "    patience=3, \n",
    "    verbose=1, \n",
    "    factor=0.5, \n",
    "    min_lr=0.00001\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Моніторинг валідаційних втрат\n",
    "    patience=5,                # Чекати 5 епох без покращення перед зупинкою\n",
    "    restore_best_weights=True,  # Відновити ваги найкращої моделі\n",
    "    mode='auto'\n",
    ")\n",
    "terminate_nan = TerminateOnNaN()\n",
    "\n",
    "# Adam optimizer\n",
    "conv_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "conv_model.fit(datagen.flow(x_combined, y_combined, batch_size=100),\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    steps_per_epoch=x_combined.shape[0] // 100,\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint, learning_rate_reduction, early_stopping, terminate_nan],\n",
    "                    shuffle=True)\n",
    "# Evaluate the model\n",
    "conv_model.evaluate(X_val, Y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-course",
   "language": "python",
   "name": "my-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
