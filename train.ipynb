{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ZeroPadding2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TerminateOnNaN, TensorBoard\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV data\n",
    "train_data = pd.read_csv(r\"D:\\Projects\\Digit-Recognizer\\train.csv\")\n",
    "\n",
    "# Prepare CSV dataset\n",
    "x_csv = train_data.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_csv = to_categorical(train_data['label'].values)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_mnist_train, y_mnist_train), (x_mnist_test, y_mnist_test) = mnist.load_data()\n",
    "x_mnist_train = x_mnist_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_train = to_categorical(y_mnist_train, 10)\n",
    "\n",
    "x_mnist_test = x_mnist_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_test = to_categorical(y_mnist_test, 10)\n",
    "\n",
    "# Concatenate CSV and MNIST datasets\n",
    "x_combined = np.concatenate([x_csv, x_mnist_train, x_mnist_test], axis=0)\n",
    "y_combined = np.concatenate([y_csv, y_mnist_train, y_mnist_test], axis=0)\n",
    "\n",
    "# Train-validation split for combined dataset\n",
    "random_seed = 2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_combined, y_combined, test_size=0.1, random_state=random_seed)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    ZeroPadding2D(padding=(1, 1), input_shape=(28, 28, 1)),  # Input layer, padding added\n",
    "    Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Activation(activations.relu),  # Activation layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),  # Max pooling layer\n",
    "    ZeroPadding2D(padding=(1, 1)),  # Zero padding layer\n",
    "    Dropout(0.2),  # Dropout layer\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Activation(activations.relu),  # Activation layer\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Max pooling layer\n",
    "    Dropout(0.2),  # Dropout layer\n",
    "    \n",
    "    Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'),  # Convolutional layer\n",
    "    BatchNormalization(),  # Batch normalization layer\n",
    "    Activation(activations.relu),  # Activation layer\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),  # Max pooling layer\n",
    "    Dropout(0.3),  # Dropout layer\n",
    "    \n",
    "    Flatten(),  # Flatten the 3D outputs to 1D\n",
    "    Dense(128, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.4),  # Dropout layer\n",
    "    Dense(10, activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Callbacks for saving the best model and learning rate reduction\n",
    "checkpoint = ModelCheckpoint(\n",
    "    r'best_model.keras', \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True, \n",
    "    verbose=1,\n",
    "    mode='auto'\n",
    ")\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', \n",
    "    patience=3, \n",
    "    verbose=1, \n",
    "    factor=0.5, \n",
    "    min_lr=0.00001\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Моніторинг валідаційних втрат\n",
    "    patience=5,                # Чекати 5 епох без покращення перед зупинкою\n",
    "    restore_best_weights=True,  # Відновити ваги найкращої моделі\n",
    "    mode='auto'\n",
    ")\n",
    "terminate_nan = TerminateOnNaN()\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./public',         # Директорія для збереження логів\n",
    "    histogram_freq=1,        # Записувати гістограми змінних після кожної епохи\n",
    "    write_graph=True,        # Записувати граф моделі\n",
    "    write_images=True,      # Записувати зображення ваг\n",
    "    update_freq='epoch'     # Оновлення на основі кожної епохи\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Image Data Generator for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "   rotation_range=10,          # Randomly rotate images by up to 10 degrees\n",
    "   zoom_range=0.1,             # Randomly zoom images by up to 10%\n",
    "   width_shift_range=0.1,      # Randomly shift images horizontally by up to 10%\n",
    "   height_shift_range=0.1,     # Randomly shift images vertically by up to 10%\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 100\n",
    "history = model.fit(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint, learning_rate_reduction, early_stopping, terminate_nan, tensorboard],\n",
    "                    shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_val, Y_val)\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential,Model\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load CSV data\n",
    "train_data = pd.read_csv(r\"D:\\Projects\\Digit-Recognizer\\train.csv\")\n",
    "\n",
    "train_data =train_data.astype('float32')\n",
    "\n",
    "# Prepare CSV dataset\n",
    "x_csv = train_data.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_csv = to_categorical(train_data['label'].values)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_mnist_train, y_mnist_train), (x_mnist_test, y_mnist_test) = mnist.load_data()\n",
    "x_mnist_train = x_mnist_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_train = to_categorical(y_mnist_train, 10)\n",
    "\n",
    "x_mnist_test = x_mnist_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_mnist_test = to_categorical(y_mnist_test, 10)\n",
    "\n",
    "# Concatenate CSV and MNIST datasets\n",
    "x_combined = np.concatenate([x_csv, x_mnist_train, x_mnist_test], axis=0)\n",
    "y_combined = np.concatenate([y_csv, y_mnist_train, y_mnist_test], axis=0)\n",
    "\n",
    "# Train-validation split for combined dataset\n",
    "random_seed = 42\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_combined, y_combined, test_size=0.1, random_state=random_seed)\n",
    "\n",
    "# Building a CNN model\n",
    "input_shape = (28,28,1)\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "# layer 1\n",
    "x = Conv2D(64,(3,3),strides=(1,1),name='layer_conv1',padding='same')(X_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2,2),name='maxPool1')(x)\n",
    "# layer 2\n",
    "x = Conv2D(32,(3,3),strides=(1,1),name='layer_conv2',padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2,2),name='maxPool2')(x)\n",
    "# layer 3\n",
    "x = Conv2D(32,(3,3),strides=(1,1),name='conv3',padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2,2), name='maxPool3')(x)\n",
    "# fc\n",
    "x = Flatten()(x)\n",
    "x = Dense(64,activation ='relu',name='fc0')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(32,activation ='relu',name='fc1')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(10,activation ='softmax',name='fc2')(x)\n",
    "\n",
    "conv_model = Model(inputs=X_input, outputs=x, name='Predict')\n",
    "\n",
    "# Callbacks for saving the best model and learning rate reduction\n",
    "checkpoint = ModelCheckpoint(\n",
    "    r'111.keras', \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True, \n",
    "    verbose=1,\n",
    "    mode='auto'\n",
    ")\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', \n",
    "    patience=3, \n",
    "    verbose=1, \n",
    "    factor=0.5, \n",
    "    min_lr=0.00001\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Моніторинг валідаційних втрат\n",
    "    patience=5,                # Чекати 5 епох без покращення перед зупинкою\n",
    "    restore_best_weights=True,  # Відновити ваги найкращої моделі\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "# Adam optimizer\n",
    "conv_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "conv_model.fit(X_train, y_combined, epochs=10, batch_size=100, validation_data=(X_val,Y_val),\n",
    "               callbacks=[checkpoint, learning_rate_reduction, early_stopping, terminate_nan, tensorboard])\n",
    "# Evaluate the model\n",
    "model.evaluate(X_val, Y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-course",
   "language": "python",
   "name": "my-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
